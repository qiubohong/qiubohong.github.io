[
  {
    "text": "今天我们来学习Transformer算法模型。",
    "startMs": 0,
    "endMs": 4547,
    "timestampMs": 0,
    "confidence": 0.95
  },
  {
    "text": "一句话核心：Transformer等于完全基于自注意力机制的序列建模引擎，",
    "startMs": 4547,
    "endMs": 10004,
    "timestampMs": 4547,
    "confidence": 0.95
  },
  {
    "text": "通过并行计算全局依赖关系，彻底取代循环神经网络RNN的串行瓶颈。",
    "startMs": 10004,
    "endMs": 15462,
    "timestampMs": 10004,
    "confidence": 0.95
  }
]