# 激活函数视频音频脚本

## 场景 1：介绍 (约 15 秒)

你知道吗？90%的 AI 模型都在用它！

今天我们来学习激活函数。

激活函数，简单理解就是神经网络的"智能开关"。

就像大脑神经元超过阈值才放电一样，激活函数决定信号是否向下传递。

---

## 场景 2：定义 (约 20 秒)

那么，激活函数到底是什么呢？

简单说，它就是神经网络的

---

## 场景 3：5 个经典激活函数 (约 35 秒)

接下来看 5 个经典激活函数！

第一个，Sigmoid。输出范围 0 到 1，适合二分类。但有梯度消失问题。

第二个，Tanh。输出范围负 1 到 1，常用于 RNN 和 LSTM。梯度消失问题依然存在。

第三个，ReLU！这是 90%现代网络的首选。公式超简单，取 x 和 0 的最大值。解决了梯度消失，但有 Dead ReLU 问题。

第四个，Leaky ReLU。解决了 Dead ReLU 问题，在负数区保留微小梯度。

第五个，Swish！Google Brain 用 AI 在 10 万种函数中找到的黑科技。精度超越 ReLU，移动端首选！

---

## 场景 4：函数性能对比 (约 18 秒)

来看性能对比！

梯度消失：Sigmoid 和 Tanh 问题严重，ReLU 系列完全没问题。

计算效率：ReLU 五星最高，Leaky ReLU 四星，Swish 三星。

精度对比：Swish 最高 95%，Leaky ReLU 92%，ReLU 90%。

结论：Swish 精度最高，ReLU 效率最优！

---

## 场景 5：动手实验 (约 15 秒)

动手实验时间！

用 Python 可视化这些函数超简单。导入 numpy 和 matplotlib，定义 x 范围，创建函数字典，画出曲线。

观察重点：

第一，Sigmoid 和 Tanh 的饱和区，两端平坦部分就是梯度消失的根源。

第二，ReLU 的负数截断，直观看到 Dead ReLU 问题。

---

## 场景 6：冷知识 (约 25 秒)

最后分享几个超酷的冷知识！

第一，神经元激活率。Sigmoid 网络只有 3 到 5%的神经元激活，太浪费了！ReLU 网络激活率高达 50%，效率爆表！

第二，Swish 的灵感来自生物。它的平滑性源于神经突触的离子通道动力学。

第三，谷歌用 AI 找函数。强化学习在 10 万种函数中发现 Swish，超越人类设计！

第四，宇宙级应用。欧洲核子中心 CERN 用 GELU 处理粒子碰撞数据，误差降低 38%！

---

## 场景 7：结尾 (约 6 秒)

使用现有的通用结尾音频：scene8-ending.mp3
