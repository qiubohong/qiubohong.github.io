#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æç¤ºè¯å·¥ç¨‹è§†é¢‘éŸ³é¢‘ç”Ÿæˆè„šæœ¬
ä½¿ç”¨Qwen3-TTSæ¨¡å‹ä»å­—å¹•æ–‡ä»¶ç”ŸæˆéŸ³é¢‘
"""

import os
import sys
import json
from pathlib import Path

import torch
import soundfile as sf
from qwen_tts import Qwen3TTSModel
import librosa
import numpy as np

# è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆpublic/PromptEngineeringï¼‰
SCRIPT_DIR = Path(os.path.dirname(os.path.abspath(__file__)))
# é¡¹ç›®æ ¹ç›®å½•ï¼ˆremotion-videosï¼‰
PROJECT_ROOT = SCRIPT_DIR.parent.parent
OUTPUT_DIR = SCRIPT_DIR

# åœºæ™¯é…ç½®ï¼šä»å­—å¹•æ–‡ä»¶è¯»å–æ–‡æœ¬
SCENES = {
    "scene1": {"name": "å¼€åœºä»‹ç»",       "caption_file": "scene1-captions.json", "output_file": "scene1-audio.mp3"},
    "scene2": {"name": "ä¸ºä»€ä¹ˆä¼šé—®å¾ˆé‡è¦", "caption_file": "scene2-captions.json", "output_file": "scene2-audio.mp3"},
    "scene3": {"name": "å››æ­¥æé—®æ³•æ€»è§ˆ",  "caption_file": "scene3-captions.json", "output_file": "scene3-audio.mp3"},
    "scene4": {"name": "ç¬¬ä¸€æ­¥ï¼šè§’è‰²",    "caption_file": "scene4-captions.json", "output_file": "scene4-audio.mp3"},
    "scene5": {"name": "ç¬¬äºŒæ­¥ï¼šèƒŒæ™¯",    "caption_file": "scene5-captions.json", "output_file": "scene5-audio.mp3"},
    "scene6": {"name": "ç¬¬ä¸‰æ­¥ï¼šä»»åŠ¡",    "caption_file": "scene6-captions.json", "output_file": "scene6-audio.mp3"},
    "scene7": {"name": "ç¬¬å››æ­¥ï¼šæ ¼å¼",    "caption_file": "scene7-captions.json", "output_file": "scene7-audio.mp3"},
    "scene8": {"name": "éšè—æŠ€å·§ï¼šæ ·ä¾‹",  "caption_file": "scene8-captions.json", "output_file": "scene8-audio.mp3"},
    "scene9": {"name": "å®Œæ•´ç¤ºä¾‹å¯¹æ¯”",    "caption_file": "scene9-captions.json", "output_file": "scene9-audio.mp3"},
    "scene10": {"name": "æ€»ç»“æ”¶å°¾",       "caption_file": "scene10-captions.json", "output_file": "scene10-audio.mp3"},
}

# Qwen3-TTSæ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹ï¼‰
_qwen_model = None

def get_qwen_model():
    """è·å–æˆ–åˆå§‹åŒ–Qwen3-TTSæ¨¡å‹"""
    global _qwen_model
    if _qwen_model is None:
        model_path = str(PROJECT_ROOT / "Qwen3-TTS-12Hz-1.7B-Base")
        print(f"ğŸ”§ åŠ è½½Qwen3-TTSæ¨¡å‹: {model_path}")
        _qwen_model = Qwen3TTSModel.from_pretrained(
            pretrained_model_name_or_path=model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return _qwen_model

def load_captions_text(caption_file: Path) -> str:
    """ä»å­—å¹•æ–‡ä»¶è¯»å–å¹¶æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬"""
    with open(caption_file, "r", encoding="utf-8") as f:
        captions = json.load(f)
    return "".join(item["text"] for item in captions)

def generate_audio(text: str, output_path: Path, max_retries: int = 3) -> bool:
    """ä½¿ç”¨Qwen3-TTSç”ŸæˆéŸ³é¢‘"""
    ref_audio = str(PROJECT_ROOT / "borfy.mp3")
    
    for attempt in range(max_retries):
        try:
            print(f"  ğŸ”„ ç”Ÿæˆè¯­éŸ³ (ç¬¬{attempt + 1}æ¬¡)...")
            model = get_qwen_model()
            wavs, sr = model.generate_voice_clone(
                ref_audio=ref_audio,
                ref_text="5åˆ†é’Ÿ AIï¼Œæ¯å¤©ææ‡‚ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼ä»Šå¤©æˆ‘ä»¬å­¦ä¹ ï¼Œ ç›‘ç£å­¦ä¹ ã€‚",
                text=text,
                language="chinese",
                max_new_tokens=512,
                do_sample=True,
                top_k=10,
                top_p=0.7,
                temperature=0.3,
                repetition_penalty=1.5,
                subtalker_dosample=True,
                subtalker_top_k=10,
                subtalker_top_p=0.7,
                subtalker_temperature=0.3,
            )
            sf.write(str(output_path), wavs[0], sr)

            # åå¤„ç†ï¼šéŸ³é‡æ ‡å‡†åŒ–
            audio, sr_loaded = librosa.load(str(output_path), sr=None)
            duration = len(audio) / sr_loaded
            print(f"  ğŸ“Š éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")

            if duration < 1.0:
                print("  âš ï¸  éŸ³é¢‘è¿‡çŸ­ï¼Œé‡è¯•...")
                continue

            if duration > 40:
                print("  âš ï¸  éŸ³é¢‘è¿‡é•¿ï¼Œè£å‰ªè‡³40ç§’...")
                audio = audio[:int(40 * sr_loaded)]

            audio = librosa.util.normalize(audio) * 0.7
            audio = librosa.effects.preemphasis(audio, coef=0.97)
            sf.write(str(output_path), audio, sr_loaded)
            print(f"  âœ… å·²ä¿å­˜: {output_path.name}")
            return True

        except Exception as e:
            print(f"  âŒ ç¬¬{attempt + 1}æ¬¡å¤±è´¥: {e}")
            import time
            time.sleep(2)

    return False

def main():
    print("=" * 60)
    print("æç¤ºè¯å·¥ç¨‹è§†é¢‘éŸ³é¢‘ç”Ÿæˆ")
    print("=" * 60)

    success_count = 0
    skipped_count = 0
    failed_count = 0

    for scene_id, scene in SCENES.items():
        caption_file = OUTPUT_DIR / scene["caption_file"]
        output_file = OUTPUT_DIR / scene["output_file"]

        print(f"\nğŸ“ [{scene_id}] {scene['name']}")

        # è·³è¿‡å·²å­˜åœ¨çš„éŸ³é¢‘
        if output_file.exists():
            print(f"  â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡: {output_file.name}")
            skipped_count += 1
            continue

        # è¯»å–å­—å¹•æ–‡æœ¬
        if not caption_file.exists():
            print(f"  âŒ å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {caption_file}")
            failed_count += 1
            continue

        text = load_captions_text(caption_file)
        print(f"  ğŸ“„ æ–‡æœ¬: {text[:60]}{'...' if len(text) > 60 else ''}")

        if generate_audio(text, output_file):
            success_count += 1
        else:
            failed_count += 1

    print("\n" + "=" * 60)
    print("éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼")
    print(f"âœ… æˆåŠŸ: {success_count}")
    print(f"â­ï¸  è·³è¿‡: {skipped_count}")
    print(f"âŒ å¤±è´¥: {failed_count}")
    print("=" * 60)

if __name__ == "__main__":
    main()