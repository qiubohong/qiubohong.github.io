---
title: 5åˆ†é’ŸAIï¼Œæ¯å¤©ææ‡‚ä¸€ä¸ªçŸ¥è¯†ç‚¹(19) - Fine-tuning æ¨¡å‹å¾®è°ƒ
date: 2026-02-19 12:00:00
toc: true
tags:
  - å­¦ä¹ æ€»ç»“
  - 5åˆ†é’ŸAI
---

> åšä¸€ä¸ªæœ‰æ¸©åº¦å’Œæœ‰å¹²è´§çš„æŠ€æœ¯åˆ†äº«ä½œè€… â€”â€” [Qborfy](https://qborfy.com)

ä»Šå¤©æˆ‘ä»¬æ¥å­¦ä¹  **Fine-tuningï¼ˆæ¨¡å‹å¾®è°ƒï¼‰**

> ä¸€å¥è¯æ ¸å¿ƒ: **Fine-tuning** æ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®è¿›è¡ŒäºŒæ¬¡è®­ç»ƒï¼Œè®©æ¨¡å‹æ›´å¥½åœ°é€‚åº”ç‰¹å®šä»»åŠ¡çš„æŠ€æœ¯ã€‚

é€šä¿—åœ°è®²ï¼Œå¦‚æœæŠŠé¢„è®­ç»ƒæ¨¡å‹æ¯”ä½œä¸€ä¸ª"é€šæ‰"å¤§å­¦ç”Ÿï¼Œé‚£ä¹ˆ Fine-tuning å°±åƒæ˜¯è®©è¿™ä¸ªå­¦ç”Ÿå»æŸä¸ªå…¬å¸å®ä¹ ï¼Œé€šè¿‡å®é™…å·¥ä½œåœºæ™¯çš„è®­ç»ƒï¼Œè®©ä»–æˆä¸ºè¿™ä¸ªé¢†åŸŸçš„"ä¸“æ‰"ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒFine-tuning ä¸æ˜¯ä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯åœ¨å·²æœ‰æ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œ"å¾®è°ƒ"ã€‚å°±åƒä½ ä¸éœ€è¦é‡æ–°å­¦ä¹ è¯­è¨€ï¼Œåªéœ€è¦å­¦ä¹ æŸä¸ªè¡Œä¸šçš„ä¸“ä¸šæœ¯è¯­å’Œå·¥ä½œæ–¹å¼ã€‚

å®ƒçš„æ ¸å¿ƒä»·å€¼åœ¨äº**è®©é€šç”¨æ¨¡å‹é€‚åº”ç‰¹å®šåœºæ™¯**ï¼šåŒ…æ‹¬æé«˜ç‰¹å®šä»»åŠ¡çš„å‡†ç¡®ç‡ã€å­¦ä¹ ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€é€‚åº”ç‰¹å®šçš„è¾“å‡ºæ ¼å¼ã€é™ä½æ¨ç†æˆæœ¬ç­‰ã€‚é€šè¿‡ Fine-tuningï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¾ƒå°çš„æˆæœ¬è·å¾—æ¥è¿‘ä¸“ç”¨æ¨¡å‹çš„æ•ˆæœã€‚

<!-- more -->

# æ˜¯ä»€ä¹ˆ

```mermaid
---
config:
  layout: elk
  look: neo
---
flowchart TB
 subgraph a["ç¥ç»ç½‘ç»œ"]
        a1["æ¿€æ´»å‡½æ•°"]
        a2["æŸå¤±å‡½æ•°"]
        a3["ç¥ç»å…ƒè®¡ç®—"]
  end
 subgraph D["Transformer"]
        d1["Encoder"]
        d2["Self-Attendtion"]
        d3["Decoder"]
  end
    0["äººå·¥æ™ºèƒ½"] --> 1["ç®—æ³•"] & 2["åº”ç”¨"]
    1 --> a & b["æœºå™¨å­¦ä¹ "]
    b --> b1["ç›‘ç£å­¦ä¹ "] & b2["æ— ç›‘ç£å­¦ä¹ "] & b3["å¼ºåŒ–å­¦ä¹ "]
    a --> A["æ·±åº¦å­¦ä¹ "]
    A --> B["å·ç§¯ç½‘ç»œ CNN"] & C["å¾ªç¯ç½‘ç»œ RNN"] & D
    D --> E["LLMå¤§æ¨¡å‹"]
    E --> e1["Tokenåˆ†è¯"] & e2["EmbeddingåµŒå…¥ç¼–ç "] & e3["Transformer å±‚å å¤„ç†"] & e4["Out Token"] & F["Function Calling"] & F1["MCPåè®®"]
    e1 --> e11["Tokenizeråˆ†è¯ç­–ç•¥"] & e12["Tokenè®¡è´¹è§„åˆ™"]
    e2 --> e21["è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰"] & e22["åµŒå…¥å±‚ï¼ˆEmbedding Layerï¼‰"] & e23["Transformerç¼–ç å™¨"] & e24["æ± åŒ–å±‚"] & e25["å½’ä¸€åŒ–è¾“å‡º"]
    F1 --> f11["MCP Server"] & f12["MCP Protocol"] & f13["MCP Client"]
    2 --> 20["AI Agent"] & 21["åœºæ™¯&äº§å“"] & 22["å¼€å‘æ¡†æ¶"]
    20 --> 201["å•å‘æ— è®°å¿†å·¥ä½œæµ"] & 202["å¾ªç¯å¯ä¸­æ–­è®°å¿†"] & 203["é€šç”¨å¤šæ¨¡æ€Agent"]
    21 --> 211["AIç¼–ç¨‹"] & 212["AIæ™ºèƒ½å®¢æœ|åŠ©æ‰‹"]
    211 --> 211a["Claude"] & 211b["Cursor"] & 211c["Codex"] & 211d["CodeBuddy(è…¾è®¯)"] & 211e["Qwen Code(é˜¿é‡Œ)"] & 221f["Gemini CLI"] & 221g["Trae(æŠ–éŸ³)"]
    211a --> 211a1["SKILLæŠ€èƒ½"] --> 211a11["SKILL.md"] & 211a12["resource & scripts"]
    212 --> 212a["æ„å›¾è¯†åˆ«"] & 212b["ReQuery"] & 212c["RAG"] & 212d["Embedding"] & 212e["Reranker"]
    b1 --> b11["Fine-tuning"]
    22 --> 221["Dify(å¯è§†åŒ–)"] & 222["Coze(å¯è§†åŒ–)"] & 223["n8n(å¯è§†åŒ–)"] & 224["Langchain"] & 225["CrewAI"] & 226["LlamaIndex"]

    0@{ shape: rounded}
     b11:::orange
    classDef orange fill:#F9B572,stroke:#FF772E
```

é€šè¿‡ä¸€å¼ å›¾æ¥ç†è§£ Fine-tuning çš„å·¥ä½œåŸç†ï¼š

```mermaid
graph TB
    subgraph é¢„è®­ç»ƒé˜¶æ®µ["ğŸŒ é¢„è®­ç»ƒé˜¶æ®µï¼šé€šç”¨èƒ½åŠ›å­¦ä¹ "]
        A1["æµ·é‡é€šç”¨æ•°æ®<br/>(äº’è”ç½‘æ–‡æœ¬ã€ä¹¦ç±ç­‰)"] --> A2["é¢„è®­ç»ƒæ¨¡å‹<br/>(GPT/BERT/LLaMA)"]
        A2 --> A3["é€šç”¨è¯­è¨€èƒ½åŠ›<br/>âœ“ è¯­æ³•ç†è§£<br/>âœ“ å¸¸è¯†æ¨ç†<br/>âœ“ åŸºç¡€å¯¹è¯"]
    end

    subgraph å¾®è°ƒé˜¶æ®µ["ğŸ¯ å¾®è°ƒé˜¶æ®µï¼šä¸“ä¸šèƒ½åŠ›è®­ç»ƒ"]
        B1["ç‰¹å®šé¢†åŸŸæ•°æ®<br/>(æ ‡æ³¨æ•°æ®é›†)"] --> B2["å†»ç»“éƒ¨åˆ†å±‚<br/>(ä¿ç•™é€šç”¨èƒ½åŠ›)"]
        B2 --> B3["è®­ç»ƒé¡¶å±‚<br/>(å­¦ä¹ ä¸“ä¸šçŸ¥è¯†)"]
        A3 -.åŸºç¡€æ¨¡å‹.-> B2
        B3 --> B4["å¾®è°ƒåæ¨¡å‹<br/>âœ“ é€šç”¨èƒ½åŠ›<br/>âœ“ ä¸“ä¸šèƒ½åŠ›"]
    end

    subgraph åº”ç”¨é˜¶æ®µ["ğŸš€ åº”ç”¨é˜¶æ®µï¼šå®é™…ä½¿ç”¨"]
        C1["ç”¨æˆ·è¾“å…¥"] --> B4
        B4 --> C2["ä¸“ä¸šè¾“å‡º<br/>(é«˜å‡†ç¡®ç‡)"]
    end

    style A2 fill:#e3f2fd,stroke:#1976d2
    style B4 fill:#fff3e0,stroke:#f57c00
    style C2 fill:#e8f5e9,stroke:#388e3c
    style é¢„è®­ç»ƒé˜¶æ®µ fill:#f5f5f5,stroke:#9e9e9e
    style å¾®è°ƒé˜¶æ®µ fill:#fff8e1,stroke:#f57f17
    style åº”ç”¨é˜¶æ®µ fill:#fafafa,stroke:#616161
```

**Fine-tuning å·¥ä½œæµç¨‹è¯´æ˜**ï¼š

è¿™ä¸ªæµç¨‹çš„æ ¸å¿ƒåœ¨äº**ä¸‰ä¸ªå…³é”®é˜¶æ®µ**ï¼š

1. **é¢„è®­ç»ƒé˜¶æ®µï¼ˆPre-trainingï¼‰**ï¼š

   - ä½¿ç”¨æµ·é‡é€šç”¨æ•°æ®è®­ç»ƒåŸºç¡€æ¨¡å‹
   - å­¦ä¹ è¯­è¨€çš„åŸºæœ¬è§„å¾‹å’Œå¸¸è¯†
   - è¿™ä¸ªé˜¶æ®µæˆæœ¬æé«˜ï¼Œé€šå¸¸ç”±å¤§å…¬å¸å®Œæˆ

2. **å¾®è°ƒé˜¶æ®µï¼ˆFine-tuningï¼‰**ï¼š

   - ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„æ ‡æ³¨æ•°æ®
   - å†»ç»“æ¨¡å‹çš„å¤§éƒ¨åˆ†å‚æ•°ï¼ˆä¿ç•™é€šç”¨èƒ½åŠ›ï¼‰
   - åªè®­ç»ƒé¡¶å±‚å‚æ•°ï¼ˆå­¦ä¹ ä¸“ä¸šçŸ¥è¯†ï¼‰
   - æˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œæ™®é€šå›¢é˜Ÿä¹Ÿèƒ½å®Œæˆ

3. **åº”ç”¨é˜¶æ®µï¼ˆInferenceï¼‰**ï¼š
   - æ¨¡å‹åŒæ—¶å…·å¤‡é€šç”¨èƒ½åŠ›å’Œä¸“ä¸šèƒ½åŠ›
   - åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½
   - å¯èƒ½æ¯”åŸå§‹æ¨¡å‹æ›´å¿«ã€æ›´ä¾¿å®œ

## Fine-tuning çš„æ ¸å¿ƒç»„æˆ

ä¸€ä¸ªå®Œæ•´çš„ Fine-tuning æµç¨‹é€šå¸¸åŒ…å«ä»¥ä¸‹å‡ ä¸ªå…³é”®éƒ¨åˆ†ï¼š

### 1. æ•°æ®å‡†å¤‡

**æ•°æ®æ ¼å¼ç¤ºä¾‹**ï¼ˆOpenAI æ ¼å¼ï¼‰ï¼š

```json
{
  "messages": [
    { "role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹" },
    { "role": "user", "content": "æˆ‘å¤´ç—›æ€ä¹ˆåŠï¼Ÿ" },
    { "role": "assistant", "content": "å¤´ç—›çš„åŸå› æœ‰å¾ˆå¤š...å»ºè®®æ‚¨..." }
  ]
}
```

- **æ•°æ®æ”¶é›†**ï¼šæ”¶é›†ç‰¹å®šé¢†åŸŸçš„å¯¹è¯æˆ–æ–‡æœ¬æ•°æ®
- **æ•°æ®æ¸…æ´—**ï¼šå»é™¤å™ªéŸ³ã€ç»Ÿä¸€æ ¼å¼
- **æ•°æ®æ ‡æ³¨**ï¼šç¡®ä¿é«˜è´¨é‡çš„è¾“å…¥-è¾“å‡ºå¯¹
- **æ•°æ®åˆ†å‰²**ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

### 2. æ¨¡å‹é€‰æ‹©

- **åŸºç¡€æ¨¡å‹**ï¼šé€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆGPT-3.5ã€LLaMAã€BERT ç­‰ï¼‰
- **æ¨¡å‹å¤§å°**ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œèµ„æºé€‰æ‹©æ¨¡å‹è§„æ¨¡
- **å¼€æº vs é—­æº**ï¼šOpenAI APIã€Hugging Face æ¨¡å‹ç­‰

### 3. è®­ç»ƒç­–ç•¥

- **å…¨é‡å¾®è°ƒï¼ˆFull Fine-tuningï¼‰**ï¼šæ›´æ–°æ‰€æœ‰å‚æ•°ï¼Œæ•ˆæœæœ€å¥½ä½†æˆæœ¬æœ€é«˜
- **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰**ï¼šåªæ›´æ–°éƒ¨åˆ†å‚æ•°ï¼Œå¦‚ LoRAã€Adapter
- **æç¤ºå¾®è°ƒï¼ˆPrompt Tuningï¼‰**ï¼šåªè®­ç»ƒæç¤ºè¯çš„åµŒå…¥å‘é‡

### 4. è¯„ä¼°ä¸ä¼˜åŒ–

- **æ€§èƒ½è¯„ä¼°**ï¼šå‡†ç¡®ç‡ã€F1 åˆ†æ•°ã€å›°æƒ‘åº¦ç­‰æŒ‡æ ‡
- **è¿‡æ‹Ÿåˆæ£€æµ‹**ï¼šç›‘æ§è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ€§èƒ½å·®å¼‚
- **è¶…å‚æ•°è°ƒä¼˜**ï¼šå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€è®­ç»ƒè½®æ•°ç­‰

## Fine-tuning vs å…¶ä»–æ–¹æ³•å¯¹æ¯”

| **ç»´åº¦**   | Prompt Engineering | RAG              | Fine-tuning       |
| ---------- | ------------------ | ---------------- | ----------------- |
| æˆæœ¬       | æä½               | ä½-ä¸­            | ä¸­-é«˜             |
| æŠ€æœ¯é—¨æ§›   | ä½                 | ä¸­               | é«˜                |
| çŸ¥è¯†æ›´æ–°   | å®æ—¶               | å®æ—¶             | éœ€è¦é‡æ–°è®­ç»ƒ      |
| é€‚ç”¨åœºæ™¯   | ç®€å•ä»»åŠ¡           | çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡   | ç‰¹å®šæ ¼å¼/é£æ ¼ä»»åŠ¡ |
| æ•ˆæœç¨³å®šæ€§ | ä¸ç¨³å®š             | è¾ƒç¨³å®š           | éå¸¸ç¨³å®š          |
| æ¨ç†é€Ÿåº¦   | æ…¢ï¼ˆé•¿æç¤ºè¯ï¼‰     | æ…¢ï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰  | å¿«                |
| æ•°æ®éœ€æ±‚   | æ—                  | æ–‡æ¡£åº“           | æ ‡æ³¨æ•°æ®é›†        |
| å¯è§£é‡Šæ€§   | é«˜                 | é«˜ï¼ˆå¯è¿½æº¯æ¥æºï¼‰ | ä½ï¼ˆé»‘ç›’ï¼‰        |

# æ€ä¹ˆåš

ä¸‹é¢æˆ‘ä»¬é€šè¿‡å‡ ä¸ªæ¡ˆä¾‹æ¥ç†è§£ Fine-tuning çš„ä½¿ç”¨åœºæ™¯å’Œå®ç°æ–¹å¼ã€‚

## æ¡ˆä¾‹ 1ï¼šå®¢æœå¯¹è¯é£æ ¼å¾®è°ƒ

è¿™æ˜¯ Fine-tuning æœ€å¸¸è§çš„åº”ç”¨åœºæ™¯ï¼Œè®©æ¨¡å‹å­¦ä¹ ç‰¹å®šçš„å¯¹è¯é£æ ¼å’Œå›ç­”æ¨¡å¼ã€‚

**åœºæ™¯æè¿°**ï¼š

- å…¬å¸æœ‰ç‰¹å®šçš„å®¢æœè¯æœ¯å’Œå›ç­”é£æ ¼
- éœ€è¦æ¨¡å‹å§‹ç»ˆä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­æ°”
- å¯¹å¸¸è§é—®é¢˜æœ‰æ ‡å‡†åŒ–çš„å›ç­”æ¨¡æ¿

**æ•°æ®å‡†å¤‡**ï¼š

```python
# training_data.jsonl
{"messages": [
    {"role": "system", "content": "ä½ æ˜¯XXå…¬å¸çš„å®¢æœåŠ©æ‰‹ï¼Œå›ç­”è¦ä¸“ä¸šã€å‹å¥½ã€ç®€æ´"},
    {"role": "user", "content": "å¦‚ä½•é€€è´§ï¼Ÿ"},
    {"role": "assistant", "content": "æ‚¨å¥½ï¼é€€è´§æµç¨‹å¦‚ä¸‹ï¼š\n1. ç™»å½•è´¦æˆ·è¿›å…¥è®¢å•é¡µé¢\n2. é€‰æ‹©éœ€è¦é€€è´§çš„å•†å“\n3. å¡«å†™é€€è´§åŸå› \n4. ç­‰å¾…å®¡æ ¸é€šè¿‡åå¯„å›å•†å“\n\nå¦‚æœ‰ç–‘é—®ï¼Œéšæ—¶è”ç³»æˆ‘ä»¬ï¼"}
]}

{"messages": [
    {"role": "system", "content": "ä½ æ˜¯XXå…¬å¸çš„å®¢æœåŠ©æ‰‹ï¼Œå›ç­”è¦ä¸“ä¸šã€å‹å¥½ã€ç®€æ´"},
    {"role": "user", "content": "å‘è´§è¦å¤šä¹…ï¼Ÿ"},
    {"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘ä»¬çš„å‘è´§æ—¶æ•ˆå¦‚ä¸‹ï¼š\n- ç°è´§å•†å“ï¼š24å°æ—¶å†…å‘è´§\n- é¢„å”®å•†å“ï¼šæŒ‰é¢„å”®è¯´æ˜å‘è´§\n- å®šåˆ¶å•†å“ï¼š7-15ä¸ªå·¥ä½œæ—¥\n\næ‚¨å¯ä»¥åœ¨è®¢å•è¯¦æƒ…ä¸­æŸ¥çœ‹å…·ä½“å‘è´§æ—¶é—´å“¦ï¼"}
]}
```

**ä½¿ç”¨ OpenAI API è¿›è¡Œå¾®è°ƒ**ï¼š

```python
from openai import OpenAI
client = OpenAI()

# 1. ä¸Šä¼ è®­ç»ƒæ•°æ®
file = client.files.create(
    file=open("training_data.jsonl", "rb"),
    purpose="fine-tune"
)

# 2. åˆ›å»ºå¾®è°ƒä»»åŠ¡
fine_tune = client.fine_tuning.jobs.create(
    training_file=file.id,
    model="gpt-3.5-turbo",
    hyperparameters={
        "n_epochs": 3,  # è®­ç»ƒè½®æ•°
        "batch_size": 4,
        "learning_rate_multiplier": 0.1
    }
)

print(f"å¾®è°ƒä»»åŠ¡ ID: {fine_tune.id}")

# 3. ç›‘æ§è®­ç»ƒè¿›åº¦
import time
while True:
    status = client.fine_tuning.jobs.retrieve(fine_tune.id)
    print(f"çŠ¶æ€: {status.status}")

    if status.status == "succeeded":
        print(f"å¾®è°ƒå®Œæˆï¼æ¨¡å‹ ID: {status.fine_tuned_model}")
        break
    elif status.status == "failed":
        print(f"å¾®è°ƒå¤±è´¥: {status.error}")
        break

    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

# 4. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹
response = client.chat.completions.create(
    model=status.fine_tuned_model,
    messages=[
        {"role": "system", "content": "ä½ æ˜¯XXå…¬å¸çš„å®¢æœåŠ©æ‰‹"},
        {"role": "user", "content": "æˆ‘æƒ³é€€è´§"}
    ]
)

print(response.choices[0].message.content)
```

## æ¡ˆä¾‹ 2ï¼šä»£ç ç”Ÿæˆæ¨¡å‹å¾®è°ƒ

**åœºæ™¯**ï¼šè®©æ¨¡å‹å­¦ä¹ å…¬å¸çš„ä»£ç è§„èŒƒå’Œå¸¸ç”¨æ¨¡å¼ã€‚

**æ•°æ®å‡†å¤‡**ï¼š

````python
# code_training_data.jsonl
{"messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª Python ä»£ç åŠ©æ‰‹ï¼Œéµå¾ª PEP 8 è§„èŒƒ"},
    {"role": "user", "content": "å†™ä¸€ä¸ªè¯»å– JSON æ–‡ä»¶çš„å‡½æ•°"},
    {"role": "assistant", "content": "```python\nimport json\nfrom typing import Dict, Any\nfrom pathlib import Path\n\ndef read_json_file(file_path: str) -> Dict[str, Any]:\n    \"\"\"è¯»å– JSON æ–‡ä»¶å¹¶è¿”å›å­—å…¸\n    \n    Args:\n        file_path: JSON æ–‡ä»¶è·¯å¾„\n        \n    Returns:\n        è§£æåçš„å­—å…¸å¯¹è±¡\n        \n    Raises:\n        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨\n        json.JSONDecodeError: JSON æ ¼å¼é”™è¯¯\n    \"\"\"\n    path = Path(file_path)\n    \n    if not path.exists():\n        raise FileNotFoundError(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n    \n    with path.open('r', encoding='utf-8') as f:\n        return json.load(f)\n```"}
]}
````

**ä½¿ç”¨ Hugging Face è¿›è¡Œå¾®è°ƒ**ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model_name = "codellama/CodeLlama-7b-hf"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. å‡†å¤‡æ•°æ®é›†
dataset = load_dataset("json", data_files="code_training_data.jsonl")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 3. è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./code-model-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    learning_rate=2e-5,
    warmup_steps=100,
    logging_steps=10,
)

# 4. åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()

# 6. ä¿å­˜æ¨¡å‹
model.save_pretrained("./code-model-final")
tokenizer.save_pretrained("./code-model-final")
```

## æ¡ˆä¾‹ 3ï¼šä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ

**LoRAï¼ˆLow-Rank Adaptationï¼‰** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°å°±èƒ½è¾¾åˆ°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚

**ä¼˜åŠ¿**ï¼š

- è®­ç»ƒé€Ÿåº¦å¿« 10 å€ä»¥ä¸Š
- æ˜¾å­˜å ç”¨å°‘ 3 å€ä»¥ä¸Š
- å¯ä»¥åŒæ—¶ç»´æŠ¤å¤šä¸ªå¾®è°ƒç‰ˆæœ¬

**å®ç°ä»£ç **ï¼š

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# 2. é…ç½® LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # LoRA ç§©ï¼Œè¶Šå¤§æ•ˆæœè¶Šå¥½ä½†è®­ç»ƒè¶Šæ…¢
    lora_alpha=32,  # LoRA ç¼©æ”¾å‚æ•°
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"],  # è¦åº”ç”¨ LoRA çš„æ¨¡å—
)

# 3. åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)

# 4. æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
# è¾“å‡ºï¼štrainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# 5. è®­ç»ƒï¼ˆä½¿ç”¨æ ‡å‡† Trainerï¼‰
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./lora-model",
    num_train_epochs=3,
    per_device_train_batch_size=8,  # LoRA å¯ä»¥ç”¨æ›´å¤§çš„ batch size
    learning_rate=3e-4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

trainer.train()

# 6. ä¿å­˜ LoRA æƒé‡ï¼ˆåªæœ‰å‡  MBï¼‰
model.save_pretrained("./lora-weights")
```

## æ¡ˆä¾‹ 4ï¼šåŒ»ç–—é—®ç­”æ¨¡å‹å¾®è°ƒ

**åœºæ™¯**ï¼šè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿå›ç­”åŒ»ç–—å¥åº·é—®é¢˜çš„ä¸“ä¸šæ¨¡å‹ã€‚

**æ•°æ®å‡†å¤‡è¦ç‚¹**ï¼š

```python
# medical_qa_data.jsonl
{"messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å¥åº·åŠ©æ‰‹ï¼Œæä¾›å‡†ç¡®ã€è´Ÿè´£ä»»çš„å¥åº·å»ºè®®"},
    {"role": "user", "content": "é«˜è¡€å‹æ‚£è€…é¥®é£Ÿè¦æ³¨æ„ä»€ä¹ˆï¼Ÿ"},
    {"role": "assistant", "content": "é«˜è¡€å‹æ‚£è€…çš„é¥®é£Ÿå»ºè®®ï¼š\n\n1. **é™åˆ¶é’ ç›æ‘„å…¥**ï¼šæ¯å¤©ä¸è¶…è¿‡6å…‹\n2. **å¢åŠ é’¾çš„æ‘„å…¥**ï¼šå¤šåƒé¦™è•‰ã€åœŸè±†ã€è èœ\n3. **æ§åˆ¶æ€»çƒ­é‡**ï¼šé¿å…è‚¥èƒ–\n4. **å‡å°‘é¥±å’Œè„‚è‚ª**ï¼šå°‘åƒåŠ¨ç‰©å†…è„ã€è‚¥è‚‰\n5. **æˆ’çƒŸé™é…’**ï¼šå¸çƒŸé¥®é…’ä¼šå‡é«˜è¡€å‹\n\nâš ï¸ é‡è¦æç¤ºï¼šä»¥ä¸Šå»ºè®®ä»…ä¾›å‚è€ƒï¼Œå…·ä½“é¥®é£Ÿæ–¹æ¡ˆè¯·å’¨è¯¢æ‚¨çš„ä¸»æ²»åŒ»ç”Ÿã€‚"}
]}
```

**å…³é”®æ³¨æ„äº‹é¡¹**ï¼š

- **æ•°æ®è´¨é‡**ï¼šåŒ»ç–—æ•°æ®å¿…é¡»ç»è¿‡ä¸“ä¸šäººå£«å®¡æ ¸
- **å…è´£å£°æ˜**ï¼šæ¨¡å‹å›ç­”è¦åŒ…å«"è¯·å’¨è¯¢åŒ»ç”Ÿ"ç­‰æç¤º
- **å®‰å…¨è¿‡æ»¤**ï¼šé¿å…æ¨¡å‹ç»™å‡ºå±é™©å»ºè®®
- **æŒç»­æ›´æ–°**ï¼šåŒ»ç–—çŸ¥è¯†æ›´æ–°å¿«ï¼Œéœ€è¦å®šæœŸé‡æ–°å¾®è°ƒ

## å®æˆ˜æŠ€å·§

### 1. æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦

```python
# âŒ é”™è¯¯ï¼šå¤§é‡ä½è´¨é‡æ•°æ®
# 10000 æ¡æ•°æ®ï¼Œä½†å¾ˆå¤šæ ¼å¼ä¸ç»Ÿä¸€ã€å›ç­”è´¨é‡å·®

# âœ… æ­£ç¡®ï¼šå°‘é‡é«˜è´¨é‡æ•°æ®
# 500 æ¡ç²¾å¿ƒæ ‡æ³¨çš„æ•°æ®ï¼Œæ ¼å¼ç»Ÿä¸€ã€å›ç­”å‡†ç¡®
```

**å»ºè®®**ï¼š

- ä» 50-100 æ¡é«˜è´¨é‡æ•°æ®å¼€å§‹
- é€æ­¥å¢åŠ åˆ° 500-1000 æ¡
- å®šæœŸå®¡æŸ¥å’Œæ¸…æ´—æ•°æ®

### 2. åˆç†è®¾ç½®è®­ç»ƒå‚æ•°

```python
# æ¨èçš„è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    num_train_epochs=3,  # ä¸€èˆ¬ 3-5 è½®è¶³å¤Ÿ
    learning_rate=2e-5,  # å­¦ä¹ ç‡ä¸è¦å¤ªå¤§
    per_device_train_batch_size=4,  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
    warmup_steps=100,  # é¢„çƒ­æ­¥æ•°
    weight_decay=0.01,  # æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
    logging_steps=10,  # æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡
    eval_steps=100,  # æ¯ 100 æ­¥è¯„ä¼°ä¸€æ¬¡
    save_steps=500,  # æ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡
)
```

### 3. ç›‘æ§è¿‡æ‹Ÿåˆ

```python
# ä½¿ç”¨éªŒè¯é›†ç›‘æ§
from sklearn.model_selection import train_test_split

train_data, val_data = train_test_split(dataset, test_size=0.1)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,  # æ·»åŠ éªŒè¯é›†
)

# è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è¯„ä¼°éªŒè¯é›†æ€§èƒ½
trainer.train()
```

### 4. ä½¿ç”¨æ—©åœï¼ˆEarly Stoppingï¼‰

```python
from transformers import EarlyStoppingCallback

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
```

# â„ï¸ å†·çŸ¥è¯†

1. **å¾®è°ƒä¸æ˜¯ä¸‡èƒ½çš„**ï¼šå¦‚æœä»»åŠ¡ä¸é¢„è®­ç»ƒæ•°æ®å·®å¼‚å¤ªå¤§ï¼ˆå¦‚åŒ»å­¦å½±åƒåˆ†æï¼‰ï¼Œå¾®è°ƒæ•ˆæœå¯èƒ½ä¸å¦‚ä»å¤´è®­ç»ƒã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ–‡æœ¬ç›¸å…³ä»»åŠ¡å¾®è°ƒæ•ˆæœæœ€å¥½ã€‚

2. **ç¾éš¾æ€§é—å¿˜ï¼ˆCatastrophic Forgettingï¼‰**ï¼šå¾®è°ƒæ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼š"å¿˜è®°"é¢„è®­ç»ƒæ—¶å­¦åˆ°çš„é€šç”¨çŸ¥è¯†ã€‚è§£å†³æ–¹æ³•æ˜¯ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œæˆ–è€…ä½¿ç”¨ LoRA ç­‰æ–¹æ³•ã€‚

3. **æ•°æ®æ ¼å¼çš„é‡è¦æ€§**ï¼šOpenAI çš„ç ”ç©¶è¡¨æ˜ï¼Œç»Ÿä¸€çš„æ•°æ®æ ¼å¼æ¯”æ•°æ®é‡æ›´é‡è¦ã€‚100 æ¡æ ¼å¼ç»Ÿä¸€çš„æ•°æ®å¯èƒ½æ¯” 1000 æ¡æ ¼å¼æ··ä¹±çš„æ•°æ®æ•ˆæœæ›´å¥½ã€‚

4. **å¾®è°ƒæˆæœ¬**ï¼š

   - OpenAI GPT-3.5 å¾®è°ƒï¼šè®­ç»ƒ $0.008/1K tokensï¼Œä½¿ç”¨ $0.012/1K tokens
   - è‡ªå·±å¾®è°ƒ LLaMA-7Bï¼šéœ€è¦ 1 å¼  A100ï¼ˆ40GBï¼‰æ˜¾å¡ï¼Œçº¦ 2-4 å°æ—¶
   - ä½¿ç”¨ LoRAï¼šåªéœ€ 1 å¼  RTX 3090ï¼ˆ24GBï¼‰ï¼Œçº¦ 1-2 å°æ—¶

5. **å¾®è°ƒ vs Prompt Engineering çš„é€‰æ‹©**ï¼š

   - å¦‚æœä»»åŠ¡å¯ä»¥é€šè¿‡ Prompt è§£å†³ï¼Œä¼˜å…ˆç”¨ Promptï¼ˆæˆæœ¬ä½ã€è¿­ä»£å¿«ï¼‰
   - å¦‚æœéœ€è¦ç‰¹å®šæ ¼å¼è¾“å‡ºã€ç‰¹å®šé£æ ¼ã€æˆ– Prompt å¤ªé•¿ï¼Œè€ƒè™‘å¾®è°ƒ
   - å¦‚æœéœ€è¦é™ä½æ¨ç†æˆæœ¬ï¼ˆé«˜é¢‘è°ƒç”¨ï¼‰ï¼Œå¾®è°ƒæ›´åˆ’ç®—

6. **å¤šä»»åŠ¡å¾®è°ƒ**ï¼šå¯ä»¥ç”¨å¤šä¸ªä»»åŠ¡çš„æ•°æ®ä¸€èµ·å¾®è°ƒï¼Œè®©æ¨¡å‹åŒæ—¶å­¦ä¼šå¤šç§èƒ½åŠ›ã€‚ä½†è¦æ³¨æ„ä»»åŠ¡ä¹‹é—´çš„å¹³è¡¡ï¼Œé¿å…æŸä¸ªä»»åŠ¡ä¸»å¯¼è®­ç»ƒã€‚

7. **æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**ï¼šè¿™æ˜¯ä¸€ç§ç‰¹æ®Šçš„å¾®è°ƒæ–¹å¼ï¼Œä½¿ç”¨å¤§é‡"æŒ‡ä»¤-å›ç­”"å¯¹è®­ç»ƒï¼Œè®©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ã€‚ChatGPT å°±æ˜¯é€šè¿‡æŒ‡ä»¤å¾®è°ƒ + RLHF è®­ç»ƒå‡ºæ¥çš„ã€‚

8. **å¾®è°ƒåçš„æ¨¡å‹å¯ä»¥ç»§ç»­å¾®è°ƒ**ï¼šä½ å¯ä»¥å…ˆç”¨é€šç”¨æ•°æ®å¾®è°ƒï¼Œå†ç”¨ç‰¹å®šæ•°æ®äºŒæ¬¡å¾®è°ƒã€‚è¿™ç§"æ¸è¿›å¼å¾®è°ƒ"æœ‰æ—¶æ•ˆæœæ›´å¥½ã€‚

# å‚è€ƒèµ„æ–™

- [OpenAI Fine-tuning å®˜æ–¹æ–‡æ¡£](https://platform.openai.com/docs/guides/fine-tuning)
- [Hugging Face Fine-tuning æ•™ç¨‹](https://huggingface.co/docs/transformers/training)
- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [PEFT åº“æ–‡æ¡£](https://huggingface.co/docs/peft/index)
- [LLaMA å¾®è°ƒå®æˆ˜](https://github.com/tloen/alpaca-lora)
- [å¾®è°ƒæœ€ä½³å®è·µ](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/)
