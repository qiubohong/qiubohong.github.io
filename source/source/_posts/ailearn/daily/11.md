---
title: 5分钟AI，每天搞懂一个知识点(11) - LLM大模型
date: 2025-08-11 21:00:00
toc: true
tags:
    - 学习总结
    - 5分钟AI
---

> 做一个有温度和有干货的技术分享作者 —— [Qborfy](https://qborfy.com)

今天我们来学习 **LLM大模型**

> 一句话核心:  LLM大模型(Large Language Model) = 基于Transformer架构的海量参数模型，通过万亿级文本训练，将人类语言规律压缩为数学表示，实现理解、生成、推理三位一体的通用智能。

5分钟AI知识点学到LLM大模型，其实基本上对AI知识点有大概的认知了，对于目前大多数接触AI的人第一个接触的肯定是LLM大模型，知道怎么用，但是不知道它是怎么来的。通过上面5分钟AI知识点学习，能够大概了解到一些脉络。

从我个人理解来讲，LLM大模型目前的定义来说，是AI技术发展到一定阶段的可实际应用的产品，有点类似电脑时代的 晶体管超级电脑（占地170平方米）发展到个人电脑时代，大家开始可以接触与应用到AI技术，不再局限于某个少数高端领域中。

<!-- more -->

# 是什么

![5分钟AI知识网络图](/assets/img/ailearn/daily/11/1.png)

**核心突破​​：**

​- **​规模效应​​：** 百亿至万亿参数（如GPT-4：1.8万亿）突破性能瓶颈
​- **​零样本学习​​：** 无需微调直接处理新任务（如翻译→摘要→代码生成）

## 组成

由于基于Transformer架构，LLM大模型本质上组成也是由：

- `编码器`
- `解码器`
- `自我注意力`

最重要一点就是利用Transfomer架构的并行处理能力，可使用非常大规模的模型，其中通常具有数千亿个参数，甚至上万亿的参数去完成模型训练。

# 怎么做

![5分钟AI知识网络图](/assets/img/ailearn/daily/11/1.png)

- **分词 Tokenization**: `BPE算法`拆解文本→Token序列（如“AI学习”→[“AI”，“学”，“习”]）
- **嵌入表示 Embedding**: 将分词Token映射为高维向量（如“猫”→[0.2, -1.3, 0.8]），捕获语义关联
- **多层Transformer堆叠**: 
  - 自注意力机制计算词间权重（如“苹果”在水果/公司语境下的不同关注度）
  - 前馈网络提炼特征（上下文关联）
- **概率预测 Next Token**: 输出下一个Token的概率分布（如“学习”后“知识”概率=92%）