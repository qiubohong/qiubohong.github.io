---
title: 5分钟AI，每天搞懂一个知识点(15) -MCP协议
date: 2025-12-17 12:00:00
toc: true
tags:
  - 学习总结
  - 5分钟AI
---

> 做一个有温度和有干货的技术分享作者 —— [Qborfy](https://qborfy.com)

今天我们来学习 **模型上下文协议（MCP）**

> **MCP（Model Context Protocol，模型上下文协议）** 是由 Anthropic 公司推出的开源协议，旨在为大型语言模型与外部数据源、工具之间建立**安全、标准化、双向**的连接通道。

通俗地说，MCP 如同 AI 世界的“USB-C 接口”。它定义了一套通用标准，使得任何支持 MCP 的 AI 应用（如 Claude Desktop、Cursor IDE）都能通过统一的“插口”，即插即用地连接各种外部工具（如数据库、API、本地文件），而无需为每个工具单独开发适配器。其核心价值在于解决了 AI 应用与外部资源交互时的碎片化集成问题，实现了生态的标准化和去中心化。

<!-- more -->

# 是什么

![5分钟AI知识网络图](/assets/img/ailearn/daily/15/1.png)

MCP 遵循经典的客户端-服务器架构，其核心组件与协作流程如下图所示，清晰地展示了数据如何在不同部分间流转。

![MCP 工作原理](/assets/img/ailearn/daily/15/2.png)

**核心组件解析**：

- **MCP Host（宿主）**：这是用户直接交互的 AI 应用程序，例如 Cursor IDE、Claude Desktop 等。它是发起所有请求的“大脑”和总指挥。
- **MCP Client（客户端）**：它内嵌于 Host 内部，充当协议的“翻译官”。每个 Client 与一个 Server 保持一对一的专有连接，负责所有通信的路由和转换。
- **MCP Server（服务器）**：这是提供具体能力的轻量级服务程序。它向外暴露三种核心能力：
  - **工具 Tool**：可执行的函数，允许 LLM 执行操作，如发送邮件、查询数据库。
  - **资源 Resource**：只读的数据源，如文件内容、数据库记录、API 响应。
  - **提示模板 Prompt**：预定义的对话模板，指导 LLM 完成特定工作流。
- **MCP Protocol（协议）**：基于**JSON-RPC 2.0**的消息格式，定义了组件间的通信规则。支持 Stdio（本地）和 HTTP（远程）两种传输方式。

# 怎么做

## MCP 服务器搭建

使用 Python 创建 MCP 服务器的简明示例，该服务器提供一个获取网页标题的工具。

```python
from fastmcp import FastMCP

mcp = FastMCP("My MCP Server")

@mcp.tool
def greet(name: str) -> str:
    return f"Hello, {name}!"

if __name__ == "__main__":
    mcp.run(transport="http", port=8000)
```

## MCP 客户端调用

使用 Python 创建 MCP 客户端的简明示例，调用上面创建的 MCP
服务器的 `get_webpage_title` 工具。

```python
import asyncio
from fastmcp import Client

client = Client("http://localhost:8000/mcp")

# 列出可用工具
async def list_tools():
    async with client:
        tools = await client.list_tools()
        print(tools)

# 调用 greet MCP tool
async def call_tool(name: str):
    async with client:
        result = await client.call_tool("greet", {"name": name})
        print(result)

asyncio.run(call_tool("Ford"))
```

## OpenAI MCP 调用

使用 OpenAI Python SDK 创建 MCP 客户端的简明示例，调用 MCP 服务器的 `greet` 工具。

```python
from openai import OpenAI

client = OpenAI()

resp = client.responses.create(
    model="gpt-5",
    tools=[
        {
            "type": "mcp",
            "server_label": "dmcp",
            "server_description": "Demo MCP Server",
            "server_url": "http://localhost:8000/mcp",
            "require_approval": "never",
        },
    ],
    input="Hello, MCP!",
)

print(resp.output_text)
```

我们在大模型中调用 MCP 服务正常步骤如下：

1. 列出 MCP 服务中可用的工具和资源。
2. 筛选出需要调用的工具或资源。
3. 构造调用请求并发送给 MCP 服务。
4. 处理 MCP 服务返回的结果。
5. 将结果添加到输入中，继续与模型对话。

# MCP 生态

- **主流 MCP Hosts（应用）**：**Cursor IDE**、**Claude Desktop**、**OpenWebUI**等是当前最流行的 MCP 宿主应用。只需在它们的设置中添加 MCP Server，即可扩展其能力。
- **MCP Server 资源库**：要寻找现成的 Server，可以访问 **`github.com/modelcontextprotocol/servers`**（官方示例）和 **`cursor.directory`**（社区目录），这里有从文件系统、数据库到各类云服务的丰富工具。
- **开发工具**：Anthropic 提供了官方的 **Python 和 TypeScript SDK**等，极大简化了 MCP Server 和 Client 的开发流程。
  - SDK 文档：[https://github.com/modelcontextprotocol](https://github.com/modelcontextprotocol)

# ❄️ 冷知识

- **灵感来源**：MCP 的设计深受**语言服务器协议（LSP）** 的启发。LSP 统一了 IDE 对不同编程语言的支持，而 MCP 旨在统一 AI 应用对不同工具和数据的访问。
- **权限控制**：在 MCP 架构下，**工具由模型控制调用，而资源（数据）的访问权完全由用户控制**。Server 所有者无需将 API 密钥等敏感信息暴露给 LLM 提供商，提升了安全性。
- **商业前景**：分析认为，MCP 在**To C（消费者）领域**（如智能硬件、社交 App）有广阔前景，因其能快速集成多样化服务。但在**To B（企业）领域**可能面临挑战，因为企业级软件更倾向于成为用户入口而非被调用的工具。

# 参考资料

- [Anthropic MCP 官方文档](https://www.anthropic.com/news/model-context-protocol)
- [Model Context Protocol(MCP) 编程极速入门](https://github.com/liaokongVFX/MCP-Chinese-Getting-Started-Guide)
- [FastMCP 官方文档](https://gofastmcp.com/getting-started/quickstart)
- [OpenAI MCP 文档](https://platform.openai.com/docs/guides/tools-connectors-mcp)
