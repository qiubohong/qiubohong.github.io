---
title: 11篇 AI从零开始 - 工业级的RAG开发与部署(1)
date: 2025-03-31 15:00:00
toc: true
tags:
    - 学习总结
    - AI学习
---

> 做一个有温度和有干货的技术分享作者 —— [Qborfy](https://qborfy.com)


经过LangGraph+LangChain系列文章的学习后， 对LangGraph有了全面，那么接下来就应该学习通过LangGraph开发工业级RAG和部署。

> 工业级服务需要达到以下几个要求：
> - 可靠性：强调高可用性和容错性，以确保业务的连续性
> - 安全性：对安全性要求极高，需要提供强大的安全保障措施，如数据加密、访问控制、安全审计等。
> - 可扩展性：需要能够根据业务发展灵活扩展，支持不断增长的用户和数据量

接下来我们一起学习如何攻克工业级RAG落地的完整方案与实现。

<!-- more -->

# 1. 前期准备

## 1.1 环境准备

### 1.1.1 开发环境

- conda，主要用于管理不同版本的python
- langgraph-cli，初始化项目的脚手架
- nodejs+react,开发智能客服系统前端环境

### 1.1.2 依赖资源环境

- 向量数据库 chroma,用于保存知识库存储
- Mysql数据库 通用知识库
- docker+docker-compose, 部署服务依赖

## 1.2 模型选择

RAG运行过程为: 知识库-> 检索 -> LLM模型

- 知识库：私有的数据，主要依赖于`embedding模型`生成存储到向量数据库中
- 检索：根据用户问题检索知识库，根据检索算法（如：ReRanker、Rewrite等）得到问题答案
- LLM模型：根据用户问题+检索知识库返回结果形成上下文，分析得到最佳答案返回给用户

### 1.2.1 embedding模型
目前主流`embedding模型`包含如下：

| **需求场景**          | **推荐模型**               | **关键优势**                          |
|----------------------|--------------------------|-------------------------------------|
| 纯中文任务           | `text2vec-large-chinese` | 中文语义理解最优                     |
| 中英混合检索         | `bge-m3`                 | 多语言支持 + 长上下文                |
| 移动端/低资源部署    | `bge-small-zh`           | 轻量高速，内存占用低                 |
| 长文档处理           | `nomic-embed-text`       | 支持 8192 tokens                     |
| 快速验证/API 集成    | `text-embedding-3-small` | 免部署，降维灵活                    |
| 企业私有化           | `m3e-large` + 本地向量库  | 数据安全 + 定制优化                 |


这里我们采用 `bge-m3`模型作为RAG的`embedding模型`，私有化部署可以参考我之前的文章[02篇 AI从零开始 - 部署本地大模型 DeepSeek-R1](https://qborfy.com/ailearn/ai-learn02.html)。

### 1.2.2 reranker重排序模型




### 1.2.3 LLM模型





# 2. 实战开发

## 2.1 LangChain Agent + Server开发


## 2.2 Formily 插件 AI 助手前端开发

# 总结


# 参考资料





