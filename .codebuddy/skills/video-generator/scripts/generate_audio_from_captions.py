#!/usr/bin/env python3
"""
åŸºäºå­—å¹•æ–‡ä»¶ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
ä½¿ç”¨Qwen3-TTSæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åœºæ™¯è§£è¯´éŸ³é¢‘

ç”¨æ³•:
    python generate_audio_from_captions.py --video-name <è§†é¢‘åç§°> --captions-dir <å­—å¹•ç›®å½•>

ç¤ºä¾‹:
    python generate_audio_from_captions.py --video-name RNNVideo --captions-dir public/RNNVideo
"""

import os
import sys
import json
import argparse
from pathlib import Path
import torch
import soundfile as sf
from qwen_tts import Qwen3TTSModel
import librosa
import numpy as np
from tqdm import tqdm

# é»˜è®¤å‚è€ƒéŸ³é¢‘é…ç½®
DEFAULT_REF_AUDIO = "./borfy.mp3"
DEFAULT_REF_TEXT = "5åˆ†é’Ÿ AIï¼Œæ¯å¤©ææ‡‚ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼ä»Šå¤©æˆ‘ä»¬å­¦ä¹ ï¼Œ ç›‘ç£å­¦ä¹ ã€‚"

# Qwen3-TTSæ¨¡å‹å®ä¾‹
_qwen_model = None


def get_qwen_model():
    """è·å–æˆ–åˆå§‹åŒ–Qwen3-TTSæ¨¡å‹"""
    global _qwen_model
    if _qwen_model is None:
        try:
            print("ğŸ”§ åŠ è½½Qwen3-TTSæ¨¡å‹...")
            
            model_kwargs = {
                "pretrained_model_name_or_path": "./Qwen3-TTS-12Hz-1.7B-Base",
                "device_map": "auto",
                "torch_dtype": torch.bfloat16,
                "low_cpu_mem_usage": True,
            }
            
            _qwen_model = Qwen3TTSModel.from_pretrained(**model_kwargs)
            print("âœ… Qwen3-TTSæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ Qwen3-TTSæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None
    return _qwen_model


def generate_tts_audio(text, output_path, ref_audio=None, ref_text=None):
    """ä½¿ç”¨Qwen3-TTSç”ŸæˆéŸ³é¢‘"""
    max_retries = 3
    ref_audio = ref_audio or DEFAULT_REF_AUDIO
    ref_text = ref_text or DEFAULT_REF_TEXT
    
    for attempt in range(max_retries):
        model = get_qwen_model()
        if model is None:
            return False
        
        print(f"ğŸ”„ å°è¯•ç”Ÿæˆè¯­éŸ³ (ç¬¬{attempt + 1}æ¬¡)...")
        
        try:
            wavs, sr = model.generate_voice_clone(
                ref_audio=ref_audio,
                ref_text=ref_text,
                text=text,
                language="chinese",
                max_new_tokens=1024,
                do_sample=True,
                top_k=10,
                top_p=0.7,
                temperature=0.3,
                repetition_penalty=1.5,
                subtalker_dosample=True,
                subtalker_top_k=10,
                subtalker_top_p=0.7,
                subtalker_temperature=0.3,
            )
            
            # ä¿å­˜éŸ³é¢‘
            sf.write(output_path, wavs[0], sr)
            
            # éŸ³é¢‘åå¤„ç†
            try:
                audio, sr_loaded = librosa.load(output_path, sr=None)
                audio_duration = len(audio) / sr_loaded
                print(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’")
                
                # éŸ³é‡æ ‡å‡†åŒ–
                audio_normalized = librosa.util.normalize(audio) * 0.7
                # ä½é€šæ»¤æ³¢æé«˜æ¸…æ™°åº¦
                audio_filtered = librosa.effects.preemphasis(audio_normalized, coef=0.97)
                
                sf.write(output_path, audio_filtered, sr_loaded)
                print(f"âœ“ éŸ³é¢‘åå¤„ç†å®Œæˆ: {output_path}")
                
                if audio_duration < 1.0:
                    print("âš ï¸  éŸ³é¢‘è¿‡çŸ­ï¼Œå¯èƒ½ç”Ÿæˆå¤±è´¥")
                    continue
                    
            except Exception as e:
                print(f"âš ï¸  éŸ³é¢‘åå¤„ç†å¤±è´¥ï¼Œä½†åŸå§‹éŸ³é¢‘å·²ä¿å­˜: {e}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç¬¬{attempt + 1}æ¬¡ç”Ÿæˆå¤±è´¥: {e}")
            if attempt < max_retries - 1:
                print("ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                import time
                time.sleep(2)
    
    return False


def extract_text_from_captions(captions_file):
    """ä»å­—å¹•æ–‡ä»¶ä¸­æå–å®Œæ•´æ–‡æœ¬"""
    with open(captions_file, 'r', encoding='utf-8') as f:
        captions = json.load(f)
    
    # æŒ‰æ—¶é—´æˆ³æ’åºå¹¶åˆå¹¶æ–‡æœ¬
    sorted_captions = sorted(captions, key=lambda x: x.get('startMs', 0))
    text = ''.join([c['text'] for c in sorted_captions])
    return text


def generate_audio_for_video(video_name, captions_dir, ref_audio=None, ref_text=None):
    """ä¸ºè§†é¢‘ç”Ÿæˆæ‰€æœ‰åœºæ™¯çš„éŸ³é¢‘"""
    captions_path = Path(captions_dir)
    
    if not captions_path.exists():
        print(f"âŒ å­—å¹•ç›®å½•ä¸å­˜åœ¨: {captions_dir}")
        return False
    
    # æŸ¥æ‰¾æ‰€æœ‰å­—å¹•æ–‡ä»¶
    caption_files = list(captions_path.glob("*-captions.json"))
    
    if not caption_files:
        print(f"âŒ æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶: {captions_dir}/*-captions.json")
        return False
    
    print(f"ğŸµ å¼€å§‹ä¸º {video_name} ç”ŸæˆéŸ³é¢‘...")
    print(f"ğŸ“ å­—å¹•ç›®å½•: {captions_dir}")
    print(f"ğŸ“‹ æ‰¾åˆ° {len(caption_files)} ä¸ªå­—å¹•æ–‡ä»¶")
    
    success_count = 0
    
    for caption_file in tqdm(sorted(caption_files), desc="ç”ŸæˆéŸ³é¢‘"):
        # ä»æ–‡ä»¶åæå–åœºæ™¯åç§°
        scene_name = caption_file.stem.replace("-captions", "")
        audio_file = captions_path / f"{scene_name}-audio.mp3"
        
        print(f"\nğŸ“ å¤„ç†åœºæ™¯: {scene_name}")
        
        # æå–å­—å¹•æ–‡æœ¬
        text = extract_text_from_captions(caption_file)
        print(f"   æ–‡æœ¬: {text[:100]}...")
        
        # ç”ŸæˆéŸ³é¢‘
        if generate_tts_audio(text, str(audio_file), ref_audio, ref_text):
            print(f"âœ… åœºæ™¯éŸ³é¢‘å®Œæˆ: {audio_file.name}")
            success_count += 1
        else:
            print(f"âŒ åœºæ™¯éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {scene_name}")
    
    print(f"\nğŸ“Š ç”Ÿæˆç»“æœ:")
    print(f"   æˆåŠŸ: {success_count}/{len(caption_files)}")
    print(f"   å¤±è´¥: {len(caption_files) - success_count}")
    
    return success_count == len(caption_files)


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    required_packages = ["torch", "transformers", "accelerate", "qwen_tts", "soundfile", "librosa", "numpy"]
    
    print("ğŸ” æ£€æŸ¥ä¾èµ–åŒ…...")
    missing = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ“ {package}")
        except ImportError:
            missing.append(package)
            print(f"âœ— {package}")
    
    if missing:
        print(f"\nâŒ ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing)}")
        print("ğŸ’¡ è¯·è¿è¡Œ: pip install -r requirement.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè¿è¡Œ")
    
    return True


def main():
    parser = argparse.ArgumentParser(
        description="åŸºäºå­—å¹•æ–‡ä»¶ç”ŸæˆéŸ³é¢‘",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    python generate_audio_from_captions.py --video-name RNNVideo --captions-dir public/RNNVideo
    python generate_audio_from_captions.py --video-name CNNVideo --captions-dir public/CNNVideo --ref-audio custom.mp3
        """
    )
    
    parser.add_argument("--video-name", required=True, help="è§†é¢‘åç§°")
    parser.add_argument("--captions-dir", required=True, help="å­—å¹•æ–‡ä»¶ç›®å½•")
    parser.add_argument("--ref-audio", default=None, help="å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--ref-text", default=None, help="å‚è€ƒéŸ³é¢‘å¯¹åº”æ–‡æœ¬")
    
    args = parser.parse_args()
    
    print("=" * 50)
    print(f"è§†é¢‘éŸ³é¢‘ç”Ÿæˆå·¥å…· - {args.video_name}")
    print("=" * 50)
    
    if not check_dependencies():
        sys.exit(1)
    
    success = generate_audio_for_video(
        args.video_name,
        args.captions_dir,
        args.ref_audio,
        args.ref_text
    )
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ éŸ³é¢‘æ–‡ä»¶ä½ç½®: {args.captions_dir}/")
        print("ğŸ¬ ä¸‹ä¸€æ­¥: è®¡ç®—éŸ³é¢‘æ—¶é•¿å¹¶æ›´æ–°å¸§æ•°")
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
